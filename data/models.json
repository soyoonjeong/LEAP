[
    {
        "Model": "CohereForAI/c4ai-command-r-08-2024",
        "#Params(B)": 32.3,
        "inference_memory(GB)": 77.52,
        "training_memory(GB)": 661.4,
        "Precision": "float16",
        "Architecture": "CohereForCausalLM",
        "hidden_size": 8192,
        "seq_length": 131072,
        "num_layers": 40,
        "num_heads": 64,
        "model_memory(GB)": 64.6,
        "optimizer_memory(GB)": 387.6,
        "gradient_memory(GB)": 129.2,
        "activation_memory(GB)": 80.0
    },
    {
        "Model": "CohereForAI/c4ai-command-r-plus-08-2024",
        "#Params(B)": 104.0,
        "inference_memory(GB)": 249.6,
        "training_memory(GB)": 2064.0,
        "Precision": "float16",
        "Architecture": "CohereForCausalLM",
        "hidden_size": 12288,
        "seq_length": 131072,
        "num_layers": 64,
        "num_heads": 96,
        "model_memory(GB)": 208.0,
        "optimizer_memory(GB)": 1248.0,
        "gradient_memory(GB)": 416.0,
        "activation_memory(GB)": 192.0
    },
    {
        "Model": "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
        "#Params(B)": 7.82,
        "inference_memory(GB)": 18.77,
        "training_memory(GB)": 157.4,
        "Precision": "float32",
        "Architecture": "ExaoneForCausalLM",
        "hidden_size": 4096,
        "seq_length": 4096,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 31.28,
        "optimizer_memory(GB)": 93.84,
        "gradient_memory(GB)": 31.28,
        "activation_memory(GB)": 1.0
    },
    {
        "Model": "Qwen/Qwen2-0.5B",
        "#Params(B)": 0.49,
        "inference_memory(GB)": 1.18,
        "training_memory(GB)": 14.07,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 896,
        "seq_length": 131072,
        "num_layers": 24,
        "num_heads": 14,
        "model_memory(GB)": 0.98,
        "optimizer_memory(GB)": 5.88,
        "gradient_memory(GB)": 1.96,
        "activation_memory(GB)": 5.25
    },
    {
        "Model": "Qwen/Qwen2-0.5B-Instruct",
        "#Params(B)": 0.49,
        "inference_memory(GB)": 1.18,
        "training_memory(GB)": 10.13,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 896,
        "seq_length": 32768,
        "num_layers": 24,
        "num_heads": 14,
        "model_memory(GB)": 0.98,
        "optimizer_memory(GB)": 5.88,
        "gradient_memory(GB)": 1.96,
        "activation_memory(GB)": 1.31
    },
    {
        "Model": "Qwen/Qwen2-1.5B",
        "#Params(B)": 1.54,
        "inference_memory(GB)": 3.7,
        "training_memory(GB)": 38.22,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 1536,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 12,
        "model_memory(GB)": 3.08,
        "optimizer_memory(GB)": 18.48,
        "gradient_memory(GB)": 6.16,
        "activation_memory(GB)": 10.5
    },
    {
        "Model": "Qwen/Qwen2-1.5B-Instruct",
        "#Params(B)": 1.54,
        "inference_memory(GB)": 3.7,
        "training_memory(GB)": 30.35,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 1536,
        "seq_length": 32768,
        "num_layers": 28,
        "num_heads": 12,
        "model_memory(GB)": 3.08,
        "optimizer_memory(GB)": 18.48,
        "gradient_memory(GB)": 6.16,
        "activation_memory(GB)": 2.62
    },
    {
        "Model": "Qwen/Qwen2-72B-Instruct-GPTQ-Int4",
        "#Params(B)": 11.9,
        "inference_memory(GB)": 28.56,
        "training_memory(GB)": 254.2,
        "Precision": "float16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 8192,
        "seq_length": 32768,
        "num_layers": 80,
        "num_heads": 64,
        "model_memory(GB)": 23.8,
        "optimizer_memory(GB)": 142.8,
        "gradient_memory(GB)": 47.6,
        "activation_memory(GB)": 40.0
    },
    {
        "Model": "Qwen/Qwen2-72B-Instruct-GPTQ-Int8",
        "#Params(B)": 21.5,
        "inference_memory(GB)": 51.6,
        "training_memory(GB)": 427.0,
        "Precision": "float16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 8192,
        "seq_length": 32768,
        "num_layers": 80,
        "num_heads": 64,
        "model_memory(GB)": 43.0,
        "optimizer_memory(GB)": 258.0,
        "gradient_memory(GB)": 86.0,
        "activation_memory(GB)": 40.0
    },
    {
        "Model": "Qwen/Qwen2-7B",
        "#Params(B)": 7.62,
        "inference_memory(GB)": 18.29,
        "training_memory(GB)": 161.66,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 3584,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 28,
        "model_memory(GB)": 15.24,
        "optimizer_memory(GB)": 91.44,
        "gradient_memory(GB)": 30.48,
        "activation_memory(GB)": 24.5
    },
    {
        "Model": "Qwen/Qwen2-7B-Instruct",
        "#Params(B)": 7.62,
        "inference_memory(GB)": 18.29,
        "training_memory(GB)": 143.28,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 3584,
        "seq_length": 32768,
        "num_layers": 28,
        "num_heads": 28,
        "model_memory(GB)": 15.24,
        "optimizer_memory(GB)": 91.44,
        "gradient_memory(GB)": 30.48,
        "activation_memory(GB)": 6.12
    },
    {
        "Model": "Qwen/Qwen2.5-0.5B",
        "#Params(B)": 0.49,
        "inference_memory(GB)": 1.18,
        "training_memory(GB)": 10.13,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 896,
        "seq_length": 32768,
        "num_layers": 24,
        "num_heads": 14,
        "model_memory(GB)": 0.98,
        "optimizer_memory(GB)": 5.88,
        "gradient_memory(GB)": 1.96,
        "activation_memory(GB)": 1.31
    },
    {
        "Model": "Qwen/Qwen2.5-0.5B-Instruct",
        "#Params(B)": 0.49,
        "inference_memory(GB)": 1.18,
        "training_memory(GB)": 10.13,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 896,
        "seq_length": 32768,
        "num_layers": 24,
        "num_heads": 14,
        "model_memory(GB)": 0.98,
        "optimizer_memory(GB)": 5.88,
        "gradient_memory(GB)": 1.96,
        "activation_memory(GB)": 1.31
    },
    {
        "Model": "Qwen/Qwen2.5-1.5B",
        "#Params(B)": 1.54,
        "inference_memory(GB)": 3.7,
        "training_memory(GB)": 38.22,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 1536,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 12,
        "model_memory(GB)": 3.08,
        "optimizer_memory(GB)": 18.48,
        "gradient_memory(GB)": 6.16,
        "activation_memory(GB)": 10.5
    },
    {
        "Model": "Qwen/Qwen2.5-1.5B-Instruct",
        "#Params(B)": 1.54,
        "inference_memory(GB)": 3.7,
        "training_memory(GB)": 30.35,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 1536,
        "seq_length": 32768,
        "num_layers": 28,
        "num_heads": 12,
        "model_memory(GB)": 3.08,
        "optimizer_memory(GB)": 18.48,
        "gradient_memory(GB)": 6.16,
        "activation_memory(GB)": 2.62
    },
    {
        "Model": "Qwen/Qwen2.5-14B",
        "#Params(B)": 14.8,
        "inference_memory(GB)": 35.52,
        "training_memory(GB)": 326.4,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 5120,
        "seq_length": 131072,
        "num_layers": 48,
        "num_heads": 40,
        "model_memory(GB)": 29.6,
        "optimizer_memory(GB)": 177.6,
        "gradient_memory(GB)": 59.2,
        "activation_memory(GB)": 60.0
    },
    {
        "Model": "Qwen/Qwen2.5-14B-Instruct",
        "#Params(B)": 14.8,
        "inference_memory(GB)": 35.52,
        "training_memory(GB)": 281.4,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 5120,
        "seq_length": 32768,
        "num_layers": 48,
        "num_heads": 40,
        "model_memory(GB)": 29.6,
        "optimizer_memory(GB)": 177.6,
        "gradient_memory(GB)": 59.2,
        "activation_memory(GB)": 15.0
    },
    {
        "Model": "Qwen/Qwen2.5-32B",
        "#Params(B)": 32.8,
        "inference_memory(GB)": 78.72,
        "training_memory(GB)": 670.4,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 5120,
        "seq_length": 131072,
        "num_layers": 64,
        "num_heads": 40,
        "model_memory(GB)": 65.6,
        "optimizer_memory(GB)": 393.6,
        "gradient_memory(GB)": 131.2,
        "activation_memory(GB)": 80.0
    },
    {
        "Model": "Qwen/Qwen2.5-32B-Instruct",
        "#Params(B)": 32.8,
        "inference_memory(GB)": 78.72,
        "training_memory(GB)": 610.4,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 5120,
        "seq_length": 32768,
        "num_layers": 64,
        "num_heads": 40,
        "model_memory(GB)": 65.6,
        "optimizer_memory(GB)": 393.6,
        "gradient_memory(GB)": 131.2,
        "activation_memory(GB)": 20.0
    },
    {
        "Model": "Qwen/Qwen2.5-3B",
        "#Params(B)": 3.09,
        "inference_memory(GB)": 7.42,
        "training_memory(GB)": 60.12,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 2048,
        "seq_length": 32768,
        "num_layers": 36,
        "num_heads": 16,
        "model_memory(GB)": 6.18,
        "optimizer_memory(GB)": 37.08,
        "gradient_memory(GB)": 12.36,
        "activation_memory(GB)": 4.5
    },
    {
        "Model": "Qwen/Qwen2.5-3B-Instruct",
        "#Params(B)": 3.09,
        "inference_memory(GB)": 7.42,
        "training_memory(GB)": 60.12,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 2048,
        "seq_length": 32768,
        "num_layers": 36,
        "num_heads": 16,
        "model_memory(GB)": 6.18,
        "optimizer_memory(GB)": 37.08,
        "gradient_memory(GB)": 12.36,
        "activation_memory(GB)": 4.5
    },
    {
        "Model": "Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4",
        "#Params(B)": 11.9,
        "inference_memory(GB)": 28.56,
        "training_memory(GB)": 254.2,
        "Precision": "float16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 8192,
        "seq_length": 32768,
        "num_layers": 80,
        "num_heads": 64,
        "model_memory(GB)": 23.8,
        "optimizer_memory(GB)": 142.8,
        "gradient_memory(GB)": 47.6,
        "activation_memory(GB)": 40.0
    },
    {
        "Model": "Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8",
        "#Params(B)": 20.8,
        "inference_memory(GB)": 49.92,
        "training_memory(GB)": 414.4,
        "Precision": "float16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 8192,
        "seq_length": 32768,
        "num_layers": 80,
        "num_heads": 64,
        "model_memory(GB)": 41.6,
        "optimizer_memory(GB)": 249.6,
        "gradient_memory(GB)": 83.2,
        "activation_memory(GB)": 40.0
    },
    {
        "Model": "Qwen/Qwen2.5-7B",
        "#Params(B)": 7.62,
        "inference_memory(GB)": 18.29,
        "training_memory(GB)": 161.66,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 3584,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 28,
        "model_memory(GB)": 15.24,
        "optimizer_memory(GB)": 91.44,
        "gradient_memory(GB)": 30.48,
        "activation_memory(GB)": 24.5
    },
    {
        "Model": "Qwen/Qwen2.5-7B-Instruct",
        "#Params(B)": 7.62,
        "inference_memory(GB)": 18.29,
        "training_memory(GB)": 143.28,
        "Precision": "bfloat16",
        "Architecture": "Qwen2ForCausalLM",
        "hidden_size": 3584,
        "seq_length": 32768,
        "num_layers": 28,
        "num_heads": 28,
        "model_memory(GB)": 15.24,
        "optimizer_memory(GB)": 91.44,
        "gradient_memory(GB)": 30.48,
        "activation_memory(GB)": 6.12
    },
    {
        "Model": "allganize/Llama-3-Alpha-Ko-8B-Instruct",
        "#Params(B)": 8.03,
        "inference_memory(GB)": 19.27,
        "training_memory(GB)": 146.54,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 4096,
        "seq_length": 8192,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 16.06,
        "optimizer_memory(GB)": 96.36,
        "gradient_memory(GB)": 32.12,
        "activation_memory(GB)": 2.0
    },
    {
        "Model": "beomi/Llama-3-KoEn-8B-Instruct-preview",
        "#Params(B)": 8.03,
        "inference_memory(GB)": 19.27,
        "training_memory(GB)": 146.54,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 4096,
        "seq_length": 8192,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 16.06,
        "optimizer_memory(GB)": 96.36,
        "gradient_memory(GB)": 32.12,
        "activation_memory(GB)": 2.0
    },
    {
        "Model": "beomi/Mistral-Ko-Inst-dev",
        "#Params(B)": 7.36,
        "inference_memory(GB)": 17.66,
        "training_memory(GB)": 140.48,
        "Precision": "bfloat16",
        "Architecture": "MistralForCausalLM",
        "hidden_size": 4096,
        "seq_length": 32768,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 14.72,
        "optimizer_memory(GB)": 88.32,
        "gradient_memory(GB)": 29.44,
        "activation_memory(GB)": 8.0
    },
    {
        "Model": "beomi/OPEN-SOLAR-KO-10.7B",
        "#Params(B)": 10.9,
        "inference_memory(GB)": 26.16,
        "training_memory(GB)": 197.7,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 4096,
        "seq_length": 4096,
        "num_layers": 48,
        "num_heads": 32,
        "model_memory(GB)": 21.8,
        "optimizer_memory(GB)": 130.8,
        "gradient_memory(GB)": 43.6,
        "activation_memory(GB)": 1.5
    },
    {
        "Model": "beomi/gemma-ko-2b",
        "#Params(B)": 2.51,
        "inference_memory(GB)": 6.02,
        "training_memory(GB)": 45.74,
        "Precision": "bfloat16",
        "Architecture": "gemmaForCausalLM",
        "hidden_size": 2048,
        "seq_length": 8192,
        "num_layers": 18,
        "num_heads": 8,
        "model_memory(GB)": 5.02,
        "optimizer_memory(GB)": 30.12,
        "gradient_memory(GB)": 10.04,
        "activation_memory(GB)": 0.56
    },
    {
        "Model": "beomi/gemma-ko-7b",
        "#Params(B)": 8.54,
        "inference_memory(GB)": 20.5,
        "training_memory(GB)": 155.03,
        "Precision": "bfloat16",
        "Architecture": "gemmaForCausalLM",
        "hidden_size": 3072,
        "seq_length": 8192,
        "num_layers": 28,
        "num_heads": 16,
        "model_memory(GB)": 17.08,
        "optimizer_memory(GB)": 102.48,
        "gradient_memory(GB)": 34.16,
        "activation_memory(GB)": 1.31
    },
    {
        "Model": "beomi/open-llama-2-ko-7b",
        "#Params(B)": 6.86,
        "inference_memory(GB)": 16.46,
        "training_memory(GB)": 123.98,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 4096,
        "seq_length": 2048,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 13.72,
        "optimizer_memory(GB)": 82.32,
        "gradient_memory(GB)": 27.44,
        "activation_memory(GB)": 0.5
    },
    {
        "Model": "google/gemma-2-27b",
        "#Params(B)": 27.2,
        "inference_memory(GB)": 65.28,
        "training_memory(GB)": 547.23,
        "Precision": "float32",
        "Architecture": "gemma2ForCausalLM",
        "hidden_size": 4608,
        "seq_length": 8192,
        "num_layers": 46,
        "num_heads": 32,
        "model_memory(GB)": 108.8,
        "optimizer_memory(GB)": 326.4,
        "gradient_memory(GB)": 108.8,
        "activation_memory(GB)": 3.23
    },
    {
        "Model": "google/gemma-2-27b-it",
        "#Params(B)": 27.2,
        "inference_memory(GB)": 65.28,
        "training_memory(GB)": 492.83,
        "Precision": "bfloat16",
        "Architecture": "gemma2ForCausalLM",
        "hidden_size": 4608,
        "seq_length": 8192,
        "num_layers": 46,
        "num_heads": 32,
        "model_memory(GB)": 54.4,
        "optimizer_memory(GB)": 326.4,
        "gradient_memory(GB)": 108.8,
        "activation_memory(GB)": 3.23
    },
    {
        "Model": "google/gemma-2-2b",
        "#Params(B)": 2.61,
        "inference_memory(GB)": 6.26,
        "training_memory(GB)": 53.11,
        "Precision": "float32",
        "Architecture": "gemma2ForCausalLM",
        "hidden_size": 2304,
        "seq_length": 8192,
        "num_layers": 26,
        "num_heads": 8,
        "model_memory(GB)": 10.44,
        "optimizer_memory(GB)": 31.32,
        "gradient_memory(GB)": 10.44,
        "activation_memory(GB)": 0.91
    },
    {
        "Model": "google/gemma-2-2b-it",
        "#Params(B)": 2.61,
        "inference_memory(GB)": 6.26,
        "training_memory(GB)": 47.89,
        "Precision": "bfloat16",
        "Architecture": "Gemma2ForCausalLM",
        "hidden_size": 2304,
        "seq_length": 8192,
        "num_layers": 26,
        "num_heads": 8,
        "model_memory(GB)": 5.22,
        "optimizer_memory(GB)": 31.32,
        "gradient_memory(GB)": 10.44,
        "activation_memory(GB)": 0.91
    },
    {
        "Model": "google/gemma-2-9b",
        "#Params(B)": 9.24,
        "inference_memory(GB)": 22.18,
        "training_memory(GB)": 187.1,
        "Precision": "float32",
        "Architecture": "gemma2ForCausalLM",
        "hidden_size": 3584,
        "seq_length": 8192,
        "num_layers": 42,
        "num_heads": 16,
        "model_memory(GB)": 36.96,
        "optimizer_memory(GB)": 110.88,
        "gradient_memory(GB)": 36.96,
        "activation_memory(GB)": 2.3
    },
    {
        "Model": "google/gemma-2-9b-it",
        "#Params(B)": 9.24,
        "inference_memory(GB)": 22.18,
        "training_memory(GB)": 168.62,
        "Precision": "bfloat16",
        "Architecture": "gemma2ForCausalLM",
        "hidden_size": 3584,
        "seq_length": 8192,
        "num_layers": 42,
        "num_heads": 16,
        "model_memory(GB)": 18.48,
        "optimizer_memory(GB)": 110.88,
        "gradient_memory(GB)": 36.96,
        "activation_memory(GB)": 2.3
    },
    {
        "Model": "meta-llama/Llama-3.1-8B",
        "#Params(B)": 8.03,
        "inference_memory(GB)": 19.27,
        "training_memory(GB)": 176.54,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 4096,
        "seq_length": 131072,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 16.06,
        "optimizer_memory(GB)": 96.36,
        "gradient_memory(GB)": 32.12,
        "activation_memory(GB)": 32.0
    },
    {
        "Model": "meta-llama/Llama-3.1-8B-Instruct",
        "#Params(B)": 8.03,
        "inference_memory(GB)": 19.27,
        "training_memory(GB)": 176.54,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 4096,
        "seq_length": 131072,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 16.06,
        "optimizer_memory(GB)": 96.36,
        "gradient_memory(GB)": 32.12,
        "activation_memory(GB)": 32.0
    },
    {
        "Model": "meta-llama/Llama-3.2-1B",
        "#Params(B)": 1.24,
        "inference_memory(GB)": 2.98,
        "training_memory(GB)": 30.32,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 2048,
        "seq_length": 131072,
        "num_layers": 16,
        "num_heads": 32,
        "model_memory(GB)": 2.48,
        "optimizer_memory(GB)": 14.88,
        "gradient_memory(GB)": 4.96,
        "activation_memory(GB)": 8.0
    },
    {
        "Model": "meta-llama/Llama-3.2-1B-Instruct",
        "#Params(B)": 1.24,
        "inference_memory(GB)": 2.98,
        "training_memory(GB)": 30.32,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 2048,
        "seq_length": 131072,
        "num_layers": 16,
        "num_heads": 32,
        "model_memory(GB)": 2.48,
        "optimizer_memory(GB)": 14.88,
        "gradient_memory(GB)": 4.96,
        "activation_memory(GB)": 8.0
    },
    {
        "Model": "meta-llama/Llama-3.2-3B",
        "#Params(B)": 3.21,
        "inference_memory(GB)": 7.7,
        "training_memory(GB)": 78.78,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 3072,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 24,
        "model_memory(GB)": 6.42,
        "optimizer_memory(GB)": 38.52,
        "gradient_memory(GB)": 12.84,
        "activation_memory(GB)": 21.0
    },
    {
        "Model": "meta-llama/Llama-3.2-3B-Instruct",
        "#Params(B)": 3.21,
        "inference_memory(GB)": 7.7,
        "training_memory(GB)": 78.78,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 3072,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 24,
        "model_memory(GB)": 6.42,
        "optimizer_memory(GB)": 38.52,
        "gradient_memory(GB)": 12.84,
        "activation_memory(GB)": 21.0
    },
    {
        "Model": "mistralai/Mistral-Nemo-Instruct-2407",
        "#Params(B)": 12.2,
        "inference_memory(GB)": 29.28,
        "training_memory(GB)": 610.22,
        "Precision": "bfloat16",
        "Architecture": "MistralForCausalLM",
        "hidden_size": 5120,
        "seq_length": 1024000,
        "num_layers": 40,
        "num_heads": 32,
        "model_memory(GB)": 24.4,
        "optimizer_memory(GB)": 146.4,
        "gradient_memory(GB)": 48.8,
        "activation_memory(GB)": 390.62
    },
    {
        "Model": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
        "#Params(B)": 4.51,
        "inference_memory(GB)": 10.82,
        "training_memory(GB)": 105.18,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 3072,
        "seq_length": 131072,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 9.02,
        "optimizer_memory(GB)": 54.12,
        "gradient_memory(GB)": 18.04,
        "activation_memory(GB)": 24.0
    },
    {
        "Model": "google/gemma-1.1-2b-it",
        "#Params(B)": 2.33,
        "inference_memory(GB)": 5.59,
        "training_memory(GB)": 42.5,
        "Precision": "bfloat16",
        "Architecture": "gemmaForCausalLM",
        "hidden_size": 2048,
        "seq_length": 8192,
        "num_layers": 18,
        "num_heads": 8,
        "model_memory(GB)": 4.66,
        "optimizer_memory(GB)": 27.96,
        "gradient_memory(GB)": 9.32,
        "activation_memory(GB)": 0.56
    },
    {
        "Model": "google/gemma-1.1-7b-it",
        "#Params(B)": 7.95,
        "inference_memory(GB)": 19.08,
        "training_memory(GB)": 144.41,
        "Precision": "bfloat16",
        "Architecture": "gemmaForCausalLM",
        "hidden_size": 3072,
        "seq_length": 8192,
        "num_layers": 28,
        "num_heads": 16,
        "model_memory(GB)": 15.9,
        "optimizer_memory(GB)": 95.4,
        "gradient_memory(GB)": 31.8,
        "activation_memory(GB)": 1.31
    },
    {
        "Model": "Bllossom/llama-3.2-Korean-Bllossom-3B",
        "#Params(B)": 2.99,
        "inference_memory(GB)": 0.0,
        "training_memory(GB)": 0.0,
        "Precision": "bfloat16",
        "Architecture": "LlamaForCausalLM",
        "hidden_size": 3072,
        "seq_length": 131072,
        "num_layers": 28,
        "num_heads": 24,
        "model_memory(GB)": 0.0,
        "optimizer_memory(GB)": 0.0,
        "gradient_memory(GB)": 0.0,
        "activation_memory(GB)": 0.0
    },
    {
        "Model": "CohereForAI/c4ai-command-r7b-12-2024",
        "#Params(B)": 7.48,
        "inference_memory(GB)": 0.0,
        "training_memory(GB)": 0.0,
        "Precision": "bfloat16",
        "Architecture": "Cohere2ForCausalLM",
        "hidden_size": 4096,
        "seq_length": 8192,
        "num_layers": 32,
        "num_heads": 32,
        "model_memory(GB)": 0.0,
        "optimizer_memory(GB)": 0.0,
        "gradient_memory(GB)": 0.0,
        "activation_memory(GB)": 0.0
    }
]